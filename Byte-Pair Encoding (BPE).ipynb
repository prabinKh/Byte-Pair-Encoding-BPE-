{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#data source\n",
        "https://www.nltk.org/nltk_data/"
      ],
      "metadata": {
        "id": "sPQFaMHsllGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Subword Tokenization in NLP\n",
        "\n"
      ],
      "metadata": {
        "id": "E8ldN9lENW3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Subword Tokenization?\n",
        "\n",
        "Subword tokenization is a technique used in Natural Language Processing (NLP) to handle the complexities of languages. It involves breaking down words into smaller units called subwords or word pieces.\n",
        "\n",
        "Limited Vocabulary Size         \n",
        "\n",
        "\n",
        "Out-of-Vocabulary (OOV) Words\n"
      ],
      "metadata": {
        "id": "C8VYXKrRPY4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "arxiv.org/html/2411.17669v1\n",
        "\n",
        "*   arxiv.org/abs/2411.08671\n",
        "*  arxiv.org/html/2406.19223v1\n",
        "\n",
        "1.   https://www.youtube.com/watch?app=desktop&v=zjaRNfvNMTs\n",
        "\n",
        "1.   arxiv.org/abs/2411.17669\n",
        "2.   List item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1HBjw68qWbYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "asTNN_TcY2cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The re module provides powerful text-searching capabilities.\n",
        "\n",
        "\n",
        "It supports matching (match, search), finding (findall, finditer), replacing (sub), and splitting (split).\n",
        "\n",
        "\n",
        "Special characters like . (any character), * (zero or more), and \\d (digits) enhance flexibility.\n",
        "\n",
        "\n",
        "Compiling patterns with re.compile() can improve performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FO2UEYHBZ-iG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#example\n"
      ],
      "metadata": {
        "id": "Vti9ulG5m662"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It remove the all the symbols\n",
        "text = \"Hello_123 ? World! \"\n",
        "match = re.findall(r\"\\w+\", text)\n",
        "print(match)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AUC-O0um9du",
        "outputId": "bd7581a5-1aac-4cf6-c4e0-25d929482340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello_123', 'World']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello_123 ? World! \"\n",
        "match = re.findall(r\"\\s+\", text)\n",
        "print(match)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOkOjOwQnV70",
        "outputId": "b385c289-a2b2-4e8c-a4d9-8226824b85d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', ' ', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"apple\"\n",
        "match = re.findall(r\"[^aeiou]\", text)  # Matches all non-vowel characters\n",
        "print(match)  # Output: ['p', 'p', 'l']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL2UncIsnnAi",
        "outputId": "216acd04-fd5d-4d8d-d304-78311869779d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['p', 'p', 'l']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CoTyc0TUnz2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = \"Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages. To remedy these issues, we propose T-Free  which directly embeds words through sparse activation patterns over character triplets, and does not require a reference corpus. T-Free inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-Free shows significant improvements in cross-lingual transfer learning.\"\n",
        "\n",
        "print(f'the original string is : ' + str(test_str))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4DcSzHXY4Nl",
        "outputId": "5c127a34-f7b8-47bf-d005-94249f255330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the original string is : Tokenizers are crucial for encoding information in Large Language Models, but their development has recently stagnated, and they contain inherent weaknesses. Major limitations include computational overhead, ineffective vocabulary use, and unnecessarily large embedding and head layers. Additionally, their performance is biased towards a reference corpus, leading to reduced effectiveness for underrepresented languages. To remedy these issues, we propose T-Free  which directly embeds words through sparse activation patterns over character triplets, and does not require a reference corpus. T-Free inherently exploits morphological similarities and allows for strong compression of embedding layers. In our exhaustive experimental evaluation, we achieve competitive downstream performance with a parameter reduction of more than 85% on these layers. Further, T-Free shows significant improvements in cross-lingual transfer learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_str= test_str.lower()"
      ],
      "metadata": {
        "id": "3VqsbSFRY4JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\w\tMatches word characters (a-z, A-Z, 0-9, _)\n",
        "\\W\tMatches non-word characters (spaces, punctuation, symbols)\n",
        "The pattern [^\\s\\w] is a negated character class, meaning it matches anything except whitespace (\\s) and word characters (\\w).\n",
        "\n",
        "Breakdown:\n",
        "[] → Defines a character class (set of characters to match).\n",
        "^ inside [] → Negation (matches anything not listed inside the brackets).\n",
        "\n",
        "\\s → Matches whitespace (spaces, tabs, newlines).\n",
        "\n",
        "\\w → Matches word characters (a-z, A-Z, 0-9, _).\n",
        "\n",
        "[^\\s\\w] → Matches any character that is NOT a whitespace or a word character."
      ],
      "metadata": {
        "id": "hLG5utoXmGdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = re.findall(r'\\w+|[^\\s\\w]+',test_str)\n",
        "print('the converted string : \\n' + str(res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxGqlrXTY4DK",
        "outputId": "5be120b6-700c-404a-a05d-8c43cc8dd1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the converted string : \n",
            "['tokenizers', 'are', 'crucial', 'for', 'encoding', 'information', 'in', 'large', 'language', 'models', ',', 'but', 'their', 'development', 'has', 'recently', 'stagnated', ',', 'and', 'they', 'contain', 'inherent', 'weaknesses', '.', 'major', 'limitations', 'include', 'computational', 'overhead', ',', 'ineffective', 'vocabulary', 'use', ',', 'and', 'unnecessarily', 'large', 'embedding', 'and', 'head', 'layers', '.', 'additionally', ',', 'their', 'performance', 'is', 'biased', 'towards', 'a', 'reference', 'corpus', ',', 'leading', 'to', 'reduced', 'effectiveness', 'for', 'underrepresented', 'languages', '.', 'to', 'remedy', 'these', 'issues', ',', 'we', 'propose', 't', '-', 'free', 'which', 'directly', 'embeds', 'words', 'through', 'sparse', 'activation', 'patterns', 'over', 'character', 'triplets', ',', 'and', 'does', 'not', 'require', 'a', 'reference', 'corpus', '.', 't', '-', 'free', 'inherently', 'exploits', 'morphological', 'similarities', 'and', 'allows', 'for', 'strong', 'compression', 'of', 'embedding', 'layers', '.', 'in', 'our', 'exhaustive', 'experimental', 'evaluation', ',', 'we', 'achieve', 'competitive', 'downstream', 'performance', 'with', 'a', 'parameter', 'reduction', 'of', 'more', 'than', '85', '%', 'on', 'these', 'layers', '.', 'further', ',', 't', '-', 'free', 'shows', 'significant', 'improvements', 'in', 'cross', '-', 'lingual', 'transfer', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem with word tokenization is that it creates a large vocabulary, which can lead to memory and processing issues (exploding vocabulary problem).\n",
        "\n",
        "To solve this, we use character tokenization, which breaks words into individual characters instead of whole words."
      ],
      "metadata": {
        "id": "oKhqyMVGoSLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "ekwoMdF3oPua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_dict = OrderedDict()\n",
        "for i in res:\n",
        "  new_string = \" \".join(char for char in i)\n",
        "  if new_string in res_dict:\n",
        "    res_dict[new_string] += 1\n",
        "  else:\n",
        "    res_dict[new_string] =1\n",
        "res_dict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWNLcIB8Y33W",
        "outputId": "a375ed6c-b87a-4480-a5e1-ed7197c5ea1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('t o k e n i z e r s', 1),\n",
              "             ('a r e', 1),\n",
              "             ('c r u c i a l', 1),\n",
              "             ('f o r', 3),\n",
              "             ('e n c o d i n g', 1),\n",
              "             ('i n f o r m a t i o n', 1),\n",
              "             ('i n', 3),\n",
              "             ('l a r g e', 2),\n",
              "             ('l a n g u a g e', 1),\n",
              "             ('m o d e l s', 1),\n",
              "             (',', 10),\n",
              "             ('b u t', 1),\n",
              "             ('t h e i r', 2),\n",
              "             ('d e v e l o p m e n t', 1),\n",
              "             ('h a s', 1),\n",
              "             ('r e c e n t l y', 1),\n",
              "             ('s t a g n a t e d', 1),\n",
              "             ('a n d', 5),\n",
              "             ('t h e y', 1),\n",
              "             ('c o n t a i n', 1),\n",
              "             ('i n h e r e n t', 1),\n",
              "             ('w e a k n e s s e s', 1),\n",
              "             ('.', 7),\n",
              "             ('m a j o r', 1),\n",
              "             ('l i m i t a t i o n s', 1),\n",
              "             ('i n c l u d e', 1),\n",
              "             ('c o m p u t a t i o n a l', 1),\n",
              "             ('o v e r h e a d', 1),\n",
              "             ('i n e f f e c t i v e', 1),\n",
              "             ('v o c a b u l a r y', 1),\n",
              "             ('u s e', 1),\n",
              "             ('u n n e c e s s a r i l y', 1),\n",
              "             ('e m b e d d i n g', 2),\n",
              "             ('h e a d', 1),\n",
              "             ('l a y e r s', 3),\n",
              "             ('a d d i t i o n a l l y', 1),\n",
              "             ('p e r f o r m a n c e', 2),\n",
              "             ('i s', 1),\n",
              "             ('b i a s e d', 1),\n",
              "             ('t o w a r d s', 1),\n",
              "             ('a', 3),\n",
              "             ('r e f e r e n c e', 2),\n",
              "             ('c o r p u s', 2),\n",
              "             ('l e a d i n g', 1),\n",
              "             ('t o', 2),\n",
              "             ('r e d u c e d', 1),\n",
              "             ('e f f e c t i v e n e s s', 1),\n",
              "             ('u n d e r r e p r e s e n t e d', 1),\n",
              "             ('l a n g u a g e s', 1),\n",
              "             ('r e m e d y', 1),\n",
              "             ('t h e s e', 2),\n",
              "             ('i s s u e s', 1),\n",
              "             ('w e', 2),\n",
              "             ('p r o p o s e', 1),\n",
              "             ('t', 3),\n",
              "             ('-', 4),\n",
              "             ('f r e e', 3),\n",
              "             ('w h i c h', 1),\n",
              "             ('d i r e c t l y', 1),\n",
              "             ('e m b e d s', 1),\n",
              "             ('w o r d s', 1),\n",
              "             ('t h r o u g h', 1),\n",
              "             ('s p a r s e', 1),\n",
              "             ('a c t i v a t i o n', 1),\n",
              "             ('p a t t e r n s', 1),\n",
              "             ('o v e r', 1),\n",
              "             ('c h a r a c t e r', 1),\n",
              "             ('t r i p l e t s', 1),\n",
              "             ('d o e s', 1),\n",
              "             ('n o t', 1),\n",
              "             ('r e q u i r e', 1),\n",
              "             ('i n h e r e n t l y', 1),\n",
              "             ('e x p l o i t s', 1),\n",
              "             ('m o r p h o l o g i c a l', 1),\n",
              "             ('s i m i l a r i t i e s', 1),\n",
              "             ('a l l o w s', 1),\n",
              "             ('s t r o n g', 1),\n",
              "             ('c o m p r e s s i o n', 1),\n",
              "             ('o f', 2),\n",
              "             ('o u r', 1),\n",
              "             ('e x h a u s t i v e', 1),\n",
              "             ('e x p e r i m e n t a l', 1),\n",
              "             ('e v a l u a t i o n', 1),\n",
              "             ('a c h i e v e', 1),\n",
              "             ('c o m p e t i t i v e', 1),\n",
              "             ('d o w n s t r e a m', 1),\n",
              "             ('w i t h', 1),\n",
              "             ('p a r a m e t e r', 1),\n",
              "             ('r e d u c t i o n', 1),\n",
              "             ('m o r e', 1),\n",
              "             ('t h a n', 1),\n",
              "             ('8 5', 1),\n",
              "             ('%', 1),\n",
              "             ('o n', 1),\n",
              "             ('f u r t h e r', 1),\n",
              "             ('s h o w s', 1),\n",
              "             ('s i g n i f i c a n t', 1),\n",
              "             ('i m p r o v e m e n t s', 1),\n",
              "             ('c r o s s', 1),\n",
              "             ('l i n g u a l', 1),\n",
              "             ('t r a n s f e r', 1),\n",
              "             ('l e a r n i n g', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT (GPT-4 and GPT-3.5)\n",
        "\n",
        "Byte Pair Encoding (BPE): These models predominantly utilize BPE for subword tokenization. It effectively balances vocabulary size, handling of rare words, and contextual understanding.\n",
        "\n",
        "\n",
        "Claude 3.7\n",
        "\n",
        "Unigram Language Model: Claude reportedly uses a Unigram Language Model tokenization. This approach calculates the probability of subword units occurring and segments the text based on the most probable sequence. It handles open vocabulary scenarios well and tends to produce shorter token sequences.\n",
        "\n",
        "\n",
        "DeepSeek R1\n",
        "\n",
        "SentencePiece (BPE Variant): DeepSeek R1 seems to favor SentencePiece, specifically a BPE-derived method. SentencePiece offers efficient encoding and handles Unicode characters effectively, making it suitable for multilingual scenarios.\n",
        "\n",
        "\n",
        "Daidu AI (ERNIE 3.0)\n",
        "\n",
        "WordPiece: WordPiece is the preferred subword tokenization for ERNIE 3.0 (the core technology behind Baidu AI). This method chooses subwords based on their likelihood of improving training data fit,"
      ],
      "metadata": {
        "id": "7JIk68pdYKo4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMILlEmkWa5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Byte-Pair Encoding (BPE)\n"
      ],
      "metadata": {
        "id": "1yDbex8jpwv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize into characters with spaces\n",
        "b a n a n a b a n d a n a\n",
        "\n",
        "1.  Count bigram occurrences\n",
        "\"b a\"\t2\n",
        "\"a n\"\t3\n",
        "\"n a\"\t3\n",
        "\"a n\"\t3\n",
        "\"n a\"\t3\n",
        "\"a b\"\t1\n",
        "\"b a\"\t2\n",
        "\"a n\"\t3\n",
        "\"n d\"\t1\n",
        "\"d a\"\t1\n",
        "\"a n\"\t3\n",
        "\"n a\"\t3\n",
        "2.  Identify the most frequent bigram\n",
        "\"a n\" appears 3 times\n",
        "\"n a\" appears 3 times\n",
        "\"b a\" appears 2 times\n",
        "\n"
      ],
      "metadata": {
        "id": "IB0aQHa_q7dO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# using chat gpt 2\n",
        "'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+'\n",
        "\n",
        "'(?:[sdmt]|ll|ve|re)\n",
        "\n",
        "': This matches a single quotation mark.\n",
        "(?:...): This is a non-capturing group, which groups the inside patterns together but doesn't store the match for later reference.\n",
        "[sdmt]: This matches one of the characters s, d, m, or t.\n",
        "|: This acts as an OR operator.\n",
        "ll|ve|re: This matches either the string ll, ve, or re. So this part of the pattern matches specific two-character words (like \"ll\", \"ve\", etc.), but only when they are enclosed in quotes.\n",
        "Effect: Matches 's, 'd, 'm, 't, 'll, 've, or 're that are surrounded by single quotes.\n",
        "\n",
        "| ?\\p{L}+\n",
        "\n",
        "|: This is another OR operator.\n",
        "?: Matches 0 or 1 occurrence of the preceding space character. So this allows for an optional space before the word.\n",
        "\\p{L}: This matches any character in the Unicode letter category (i.e., any alphabetic letter from any language). This uses Unicode property escapes, which are supported in libraries like regex but not in Python's built-in re.\n",
        "+: This matches 1 or more occurrences of the previous character (which is \\p{L}), so it will match entire words (one or more letters).\n",
        "Effect: Matches a word consisting of alphabetic characters, allowing for an optional space before it.\n",
        "\n",
        "| ?\\p{N}+\n",
        "\n",
        "|: Another OR operator.\n",
        "?: Again, matches 0 or 1 occurrence of a space before the number.\n",
        "\\p{N}: This matches any character in the Unicode number category (i.e., any numeric digit). It will match digits from any script or language.\n",
        "+: Matches 1 or more occurrences of digits.\n",
        "Effect: Matches a number consisting of numeric characters, allowing for an optional space before it.\n",
        "\n",
        "| ?[^\\s\\p{L}\\p{N}]+\n",
        "\n",
        "|: OR operator.\n",
        "?: Matches 0 or 1 occurrence of the preceding space character.\n",
        "[^\\s\\p{L}\\p{N}]: This is a negated character class, meaning it will match any character that is not:\n",
        "\\s: a whitespace character (space, tab, newline, etc.)\n",
        "\\p{L}: a letter (any alphabetic character)\n",
        "\\p{N}: a digit (any numeric character)\n",
        "+: Matches 1 or more occurrences of such non-alphabetic, non-numeric, and non-whitespace characters. This can be punctuation marks or other special characters.\n",
        "Effect: Matches special characters or punctuation marks (e.g., ,, ., ?) that are not spaces, letters, or numbers, and it allows an optional space before them.\n",
        "\n",
        "|\\s+(?!\\S)\n",
        "\n",
        "|: OR operator.\n",
        "\\s+: Matches 1 or more whitespace characters (spaces, tabs, newlines, etc.).\n",
        "(?!\\S): This is a negative lookahead assertion. It asserts that what follows the whitespace is not a non-whitespace character (\\S stands for non-whitespace characters). This essentially means it matches whitespace at the end of the string or when followed by another space.\n",
        "Effect: Matches spaces or newlines, but only if they are not followed by any non-whitespace characters. This ensures that only trailing spaces or isolated spaces between words are matched.\n",
        "\n",
        "|\\s+\n",
        "\n",
        "\\s+: Matches 1 or more whitespace characters (spaces, tabs, newlines, etc.).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mNQYU07_z2nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'(?:[sdmt]|ll|ve|re)\n",
        "\n",
        "Matches single quote followed by one of the specified abbreviations (s, d, m, t, ll, ve, re) and then a closing quote.\n",
        "Example Match:\n",
        "'s, 'll, 've, 're\n",
        "Text: \"This is 's example, 'll be there soon, 've been waiting\"\n",
        "\n",
        "\n",
        "| ?\\p{L}+\n",
        "\n",
        "Matches a word made up of one or more alphabetic characters, possibly preceded by an optional space.\n",
        "Example Match:\n",
        "hello, world, Python, example\n",
        "Text: \"I love Python and the world\"\n",
        "\n",
        "\n",
        "| ?\\p{N}+\n",
        "\n",
        "Matches a number, possibly preceded by an optional space.\n",
        "Example Match:\n",
        "123, 456, 7890\n",
        "Text: \"The cost is 500 dollars, and there are 3 apples\"\n",
        "\n",
        "\n",
        "| ?[^\\s\\p{L}\\p{N}]+\n",
        "\n",
        "Matches a special character or punctuation mark that is not a letter, number, or whitespace.\n",
        "Example Match:\n",
        ",, ., !, ?, ;, :, @, #\n",
        "Text: \"Hello, how are you? I am good!\"\n",
        "\n",
        "\n",
        "|\\s+(?!\\S)\n",
        "\n",
        "Matches whitespace characters (spaces, tabs, newlines), ensuring they are not followed by any non-whitespace character (i.e., trailing spaces or newlines).\n",
        "Example Match:\n",
        "(spaces), \\t (tab), \\n (newline)\n",
        "Text: \"This is a sentence followed by trailing spaces \"\n",
        "\n",
        "|\\s+\n",
        "\n",
        "Matches one or more whitespace characters, including spaces, tabs, and newlines, anywhere in the string.\n",
        "Example Match:\n",
        "(spaces), \\t (tab), \\n (newline)\n",
        "Text: \"I love programming and coding\""
      ],
      "metadata": {
        "id": "tfxK19WOB_cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import pattern\n",
        "import regex as re\n",
        "def process_string_with_regex(text):\n",
        "  pattern = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "  tokens = re.findall(pattern,text)\n",
        "  return tokens\n",
        "\n",
        "def get_word_frequencies(tokens):\n",
        "  res_dict ={}\n",
        "  for word in tokens:\n",
        "    word = \" \".join(word)\n",
        "    res_dict[word] = res_dict.get(word,0)+1\n",
        "  return res_dict\n",
        "\n",
        "\n",
        "def get_stats(vocab):\n",
        "  pairs ={}\n",
        "  for word,freq in vocab.items():\n",
        "    symbols = word.split()\n",
        "    for i in range(len(symbols)-1):\n",
        "      pair =(symbols[i],symbols[i+1])\n",
        "      pairs[pair] = pairs.get(pair,0) + freq\n",
        "\n",
        "  return pairs\n",
        "def mearge_vocab(pair,vocab):\n",
        "  pattern = re.compile(r'\\b'+re.escape(' '.join(pair))+r'\\b')\n",
        "  return {pattern.sub(''.join(pair),word):freq for word,freq in vocab.items()}\n",
        "\n",
        "\n",
        "def byte_pair_encoding(vocab):\n",
        "  while True:\n",
        "    pairs = get_stats(vocab)\n",
        "    if not pairs:\n",
        "      break\n",
        "    best_pair = max(pairs,key=pairs.get)\n",
        "    vocab = mearge_vocab(best_pair,vocab)\n",
        "  return vocab\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_string(text):\n",
        "  tokens = process_string_with_regex(text)\n",
        "  res_dict = get_word_frequencies(tokens)\n",
        "  vocab = byte_pair_encoding(res_dict)\n",
        "  return(vocab)\n",
        "text = \"This is 's example, 'll be there soon, 've been waiting  ,   I love Python and the world  ,   The cost is 500 dollars, and there are 3 apples Hello, how are you? I am good! This is a sentence followed by trailing spaces I love programming and coding\"\n",
        "result = process_string(text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tUiCsgerUZi",
        "outputId": "148b5d5a-fa21-4967-9896-dffbb5bf1397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'This': 1, '  is': 3, \"  '\": 3, 's': 1, '  example': 1, ',': 4, 'll': 1, '  be': 1, '  there': 2, '  soon': 1, 've': 1, '  been': 1, '  waiting': 1, ' ': 2, '  ,': 2, '   ': 2, '  I': 3, '  love': 2, '  Python': 1, '  and': 3, '  the': 1, '  world': 1, '  The': 1, '  cost': 1, '  500': 1, '  dollars': 1, '  are': 2, '  3': 1, '  apples': 1, '  Hello': 1, '  how': 1, '  you': 1, '?': 1, '  am': 1, '  good': 1, '!': 1, '  This': 1, '  a': 1, '  sentence': 1, '  followed': 1, '  by': 1, '  trailing': 1, '  spaces': 1, '  programming': 1, '  coding': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VHsfXPy5DRob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVvvkUnoH7Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentencepiece python module\n",
        "\n",
        "https://arxiv.org/pdf/1808.06226\n",
        "\n",
        "python/sentencepiece_python_module_example.ipynb"
      ],
      "metadata": {
        "id": "tAd6GUNmH8nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "entencePiece is a data-driven, language-independent subword tokenization method developed by Google. It is designed to handle raw text and is particularly useful for multilingual tasks and low-resource languages. It is often used in machine translation, text generation, and other NLP tasks that require robust tokenization. SentencePiece models are based on the idea of subword units, allowing the model to handle rare words, compound words, and out-of-vocabulary (OOV) words efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "-VfS1J3BNwmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osqtfegkLTcM",
        "outputId": "72d9e59b-4f9b-457d-8fbc-faa643bed669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "--2025-03-20 06:11:44--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt’\n",
            "\n",
            "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-20 06:11:44 (10.8 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=3000')\n"
      ],
      "metadata": {
        "id": "BTsY8EH3H7PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "sp.encode_as_pieces('hello world basic end-to-end example')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdBYHAmJH7Ks",
        "outputId": "8305343a-520c-4a26-9bfc-40ff16e6238e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁he',\n",
              " 'll',\n",
              " 'o',\n",
              " '▁world',\n",
              " '▁bas',\n",
              " 'ic',\n",
              " '▁end',\n",
              " '-',\n",
              " 'to',\n",
              " '-',\n",
              " 'en',\n",
              " 'd',\n",
              " '▁example']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.encode_as_pieces('This is a test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIXcrYE_Mri7",
        "outputId": "c8f5cb97-89dc-4402-9d03-ce3c999d25a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁This', '▁is', '▁a', '▁t', 'est']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.encode_as_ids('This is a test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Wd_snOMydx",
        "outputId": "e8f54970-9345-47a0-ee8a-af1808f9e04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[179, 28, 8, 636, 510]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sp.get_piece_size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJELJsD0QCk3",
        "outputId": "471dc0b0-a8b1-4dc3-8113-f5c3941dbb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp.id_to_piece(209)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ecyS9R6qPYlk",
        "outputId": "0b0def83-fef5-4c09-a9db-03db518dea44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'▁heard'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qhjV3E8vNbmH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbpzlLczNSuF",
        "outputId": "212756a4-9e17-4966-daa2-45792ab3be80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "647"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "sp.piece_to_id('This')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for id in range(3):\n",
        "  print(sp.id_to_piece(id),sp.is_control(id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHbP2R5wdAaR",
        "outputId": "5e14547f-7e58-4dd4-e65c-27dd7bc86375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_pieces('hello world',-1,0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5JHSelidDd8",
        "outputId": "e8f61a52-3bac-43e5-928c-b3baf4e9847b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', 'h', 'e', 'll', 'o', '▁', 'w', 'o', 'r', 'ld']\n",
            "['▁he', 'll', 'o', '▁w', 'or', 'ld']\n",
            "['▁', 'h', 'e', 'll', 'o', '▁', 'w', 'or', 'l', 'd']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁', 'h', 'el', 'l', 'o', '▁wor', 'ld']\n",
            "['▁he', 'll', 'o', '▁world']\n",
            "['▁he', 'll', 'o', '▁w', 'or', 'ld']\n",
            "['▁', 'h', 'e', 'll', 'o', '▁world']\n",
            "['▁', 'h', 'el', 'l', 'o', '▁wor', 'l', 'd']\n",
            "['▁he', 'l', 'l', 'o', '▁world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(10):\n",
        "  print(sp.sample_encode_as_ids('hello world', -1, 0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYpzreYxd7he",
        "outputId": "3628ee20-0c9a-4530-cccd-f92120109691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15, 177, 347, 110, 80, 15, 205, 80, 63, 110, 34]\n",
            "[15, 1071, 119, 80, 15, 984, 63, 110, 34]\n",
            "[26, 119, 80, 396, 80, 63, 1157]\n",
            "[26, 110, 110, 80, 1093, 1157]\n",
            "[26, 119, 80, 1093, 1157]\n",
            "[26, 119, 80, 938]\n",
            "[15, 1071, 119, 80, 1093, 1157]\n",
            "[26, 110, 110, 80, 396, 168, 1157]\n",
            "[15, 1071, 110, 110, 80, 396, 80, 63, 1157]\n",
            "[15, 1071, 119, 80, 15, 984, 63, 1157]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE (Byte pair encoding) model\n"
      ],
      "metadata": {
        "id": "4SueV5s7em0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe')\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "print('*** BPE ***')\n",
        "print(sp_bpe.encode_as_pieces('thisisatesthelloworld'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_vYA2QoekYD",
        "outputId": "be14dce7-7bcc-4f9e-d26b-be59181be51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** BPE ***\n",
            "['▁this', 'is', 'at', 'est', 'he', 'llow', 'or', 'ld']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tocode(s):\n",
        "    out = []\n",
        "    for c in s:\n",
        "        out.append(str(hex(ord(c))).replace('0x', 'U+'))\n",
        "    return ' '.join(out)\n",
        "\n",
        "\n",
        "# TSV format:  source Unicode code points  target code points\n",
        "# normalize \"don't => do not,  I'm => I am\"\n",
        "with open('normalization_rule.tsv', 'w') as f:\n",
        "  f.write(tocode(\"I'm\") + '\\t' + tocode(\"I am\") + '\\n')\n",
        "  f.write(tocode(\"don't\") + '\\t' + tocode(\"do not\") + '\\n')\n",
        "\n",
        "print(open('normalization_rule.tsv', 'r').read())\n",
        "\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=2000 --normalization_rule_tsv=normalization_rule.tsv')\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "# m.model embeds the normalization rule compiled into an FST.\n",
        "sp.load('m.model')\n",
        "print(sp.encode_as_pieces(\"I'm busy\"))  # normalized to `I am busy'\n",
        "print(sp.encode_as_pieces(\"I don't know it.\"))  # normalized to 'I do not know it.'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LggmIyx5goKO",
        "outputId": "793d9a5f-64a5-4fa5-da5f-aba39365a5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U+49 U+27 U+6d\tU+49 U+20 U+61 U+6d\n",
            "U+64 U+6f U+6e U+27 U+74\tU+64 U+6f U+20 U+6e U+6f U+74\n",
            "\n",
            "['▁I', '▁am', '▁bu', 's', 'y']\n",
            "['▁I', '▁do', '▁not', '▁know', '▁it', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#What are Stop words?\n"
      ],
      "metadata": {
        "id": "28Fy012xi2eJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are commonly used words (such as \"is,\" \"the,\" \"and,\" \"in,\" \"of\") that are filtered out during text processing because they do not add much meaning to a sentence. Removing stop words helps reduce dimensionality and improves the efficiency of Natural Language Processing (NLP) models.\n",
        "\n",
        "so we are removing the stop words"
      ],
      "metadata": {
        "id": "yog7YBOgjcOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "J50idV9Ti3iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2L7nfrhjjWY",
        "outputId": "f5d27951-2775-479f-e167-52b222bd9738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = stopwords.words('english')\n",
        "print(english_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeDeHtgtkPyX",
        "outputId": "2e9c295e-7248-4f81-d2d5-19aae5f5d2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nepali_stopwords= stopwords.words('nepali')\n",
        "print(nepali_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faJj6nD5ln98",
        "outputId": "445613cb-b652-4c2f-a165-683dd511b052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['छ', 'र', 'पनि', 'छन्', 'लागि', 'भएको', 'गरेको', 'भने', 'गर्न', 'गर्ने', 'हो', 'तथा', 'यो', 'रहेको', 'उनले', 'थियो', 'हुने', 'गरेका', 'थिए', 'गर्दै', 'तर', 'नै', 'को', 'मा', 'हुन्', 'भन्ने', 'हुन', 'गरी', 'त', 'हुन्छ', 'अब', 'के', 'रहेका', 'गरेर', 'छैन', 'दिए', 'भए', 'यस', 'ले', 'गर्नु', 'औं', 'सो', 'त्यो', 'कि', 'जुन', 'यी', 'का', 'गरि', 'ती', 'न', 'छु', 'छौं', 'लाई', 'नि', 'उप', 'अक्सर', 'आदि', 'कसरी', 'क्रमशः', 'चाले', 'अगाडी', 'अझै', 'अनुसार', 'अन्तर्गत', 'अन्य', 'अन्यत्र', 'अन्यथा', 'अरु', 'अरुलाई', 'अर्को', 'अर्थात', 'अर्थात्', 'अलग', 'आए', 'आजको', 'ओठ', 'आत्म', 'आफू', 'आफूलाई', 'आफ्नै', 'आफ्नो', 'आयो', 'उदाहरण', 'उनको', 'उहालाई', 'एउटै', 'एक', 'एकदम', 'कतै', 'कम से कम', 'कसै', 'कसैले', 'कहाँबाट', 'कहिलेकाहीं', 'का', 'किन', 'किनभने', 'कुनै', 'कुरा', 'कृपया', 'केही', 'कोही', 'गए', 'गरौं', 'गर्छ', 'गर्छु', 'गर्नुपर्छ', 'गयौ', 'गैर', 'चार', 'चाहनुहुन्छ', 'चाहन्छु', 'चाहिए', 'छू', 'जताततै', 'जब', 'जबकि', 'जसको', 'जसबाट', 'जसमा', 'जसलाई', 'जसले', 'जस्तै', 'जस्तो', 'जस्तोसुकै', 'जहाँ', 'जान', 'जाहिर', 'जे', 'जो', 'ठीक', 'तत्काल', 'तदनुसार', 'तपाईको', 'तपाई', 'पर्याप्त', 'पहिले', 'पहिलो', 'पहिल्यै', 'पाँच', 'पाँचौं', 'तल', 'तापनी', 'तिनी', 'तिनीहरू', 'तिनीहरुको', 'तिनिहरुलाई', 'तिमी', 'तिर', 'तीन', 'तुरुन्तै', 'तेस्रो', 'तेस्कारण', 'पूर्व', 'प्रति', 'प्रतेक', 'प्लस', 'फेरी', 'बने', 'त्सपछि', 'त्सैले', 'त्यहाँ', 'थिएन', 'दिनुभएको', 'दिनुहुन्छ', 'दुई', 'देखि', 'बरु', 'बारे', 'बाहिर', 'देखिन्छ', 'देखियो', 'देखे', 'देखेको', 'देखेर', 'दोस्रो', 'धेरै', 'नजिकै', 'नत्र', 'नयाँ', 'निम्ति', 'बाहेक', 'बीच', 'बीचमा', 'भन', 'निम्न', 'निम्नानुसार', 'निर्दिष्ट', 'नौ', 'पक्का', 'पक्कै', 'पछि', 'पछिल्लो', 'पटक', 'पर्छ', 'पर्थ्यो', 'भन्छन्', 'भन्', 'भन्छु', 'भन्दा', 'भन्नुभयो', 'भर', 'भित्र', 'भित्री', 'म', 'मलाई', 'मात्र', 'माथि', 'मुख्य', 'मेरो', 'यति', 'यथोचित', 'यदि', 'यद्यपि', 'यसको', 'यसपछि', 'यसबाहेक', 'यसरी', 'यसो', 'यस्तो', 'यहाँ', 'यहाँसम्म', 'या', 'रही', 'राखे', 'राख्छ', 'राम्रो', 'रूप', 'लगभग', 'वरीपरी', 'वास्तवमा', 'बिरुद्ध', 'बिशेष', 'सायद', 'शायद', 'संग', 'संगै', 'सक्छ', 'सट्टा', 'सधै', 'सबै', 'सबैलाई', 'समय', 'सम्भव', 'सम्म', 'सही', 'साँच्चै', 'सात', 'साथ', 'साथै', 'सारा', 'सोही', 'स्पष्ट', 'हरे', 'हरेक']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= \" the cat is sitting on the mat .\"\n",
        "words = text.split()\n",
        "\n",
        "# filtered_text = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "# print(filtered_text)\n",
        "\n",
        "\n",
        "# simple form\n",
        "filtered_text = []\n",
        "for w in words:\n",
        "    if w.lower() not in stopwords.words('english'):\n",
        "        filtered_text.append(w)\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrUfitthjtw_",
        "outputId": "55e7f23e-2ac0-433c-c175-78730e65a322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'sitting', 'mat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyXlm1RKkMuW",
        "outputId": "81468367-e68d-492e-cc52-d8fdb3c59513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sample text\n",
        "text_data = \"\"\"Natural Language Processing (NLP) is a fascinating field of artificial intelligence.\n",
        "          It helps computers understand, interpret, and generate human language efficiently.\n",
        "          NLP applications include chatbots, sentiment analysis, machine translation, and more.\"\"\"\n",
        "\n",
        "\n",
        "word = text_data.split()\n",
        "# Convert words to lowercase and remove stopwords & punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words =[word.lower() for word in words if word.lower()]\n",
        "# Count word frequency\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "# Find the most common word\n",
        "most_common_word, most_common_count = word_freq.most_common(1)[0]\n",
        "\n",
        "# Output results\n",
        "print(\"Filtered words (without stopwords and punctuation):\", filtered_words)\n",
        "print(\"Word Frequency:\", word_freq)\n",
        "print(f\"The most common non-stopword is '{most_common_word}' with a frequency of {most_common_count}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OznQMwEom316",
        "outputId": "d2e46688-7d51-4781-c720-5024b79c05d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered words (without stopwords and punctuation): ['natural', 'language', 'processing', '(nlp)', 'fascinating', 'field', 'artificial', 'intelligence.', 'helps', 'computers', 'understand,', 'interpret,', 'generate', 'human', 'language', 'efficiently.', 'nlp', 'applications', 'include', 'chatbots,', 'sentiment', 'analysis,', 'machine', 'translation,', 'more.']\n",
            "Word Frequency: Counter({'language': 2, 'natural': 1, 'processing': 1, '(nlp)': 1, 'fascinating': 1, 'field': 1, 'artificial': 1, 'intelligence.': 1, 'helps': 1, 'computers': 1, 'understand,': 1, 'interpret,': 1, 'generate': 1, 'human': 1, 'efficiently.': 1, 'nlp': 1, 'applications': 1, 'include': 1, 'chatbots,': 1, 'sentiment': 1, 'analysis,': 1, 'machine': 1, 'translation,': 1, 'more.': 1})\n",
            "The most common non-stopword is 'language' with a frequency of 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization & Stemming\n",
        "\n",
        "# but it is stemming and lemmatization donot use it is traditional now we are using the subword tokenization (BPE,WordPiece, sentencepiece)\n",
        "What is Stemming?\n",
        "Stemming reduces words to their root form by chopping off suffixes but it may not always produce meaningful words.\n",
        "\n",
        "Example: \"running\" → \"run\", \"better\" → \"bet\"\n",
        "\n",
        "\n",
        "What is Lemmatization?\n",
        "Lemmatization reduces words to their base form using vocabulary and grammar rules, resulting in real words.\n",
        "\n",
        "Example: \"running\" → \"run\", \"better\" → \"good\""
      ],
      "metadata": {
        "id": "YTBxDLuiamt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "dhMOPHIwqdS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "y7Mbq4CRbpQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('own-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEpv5VPRb3so",
        "outputId": "80061dcf-3cc1-46c5-c04c-014da556430c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Error loading own-1.4: Package 'own-1.4' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer= PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "q2jcirDAb90U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\", \"flies\", \"better\", \"cats\", \"driving\", \"mice\"]\n",
        "for word in words:\n",
        "  print(f\"{word} ---->  {stemmer.stem(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCp7zoCgcGBb",
        "outputId": "fc71bef4-f87e-4241-f270-79579d3cae72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running ---->  run\n",
            "flies ---->  fli\n",
            "better ---->  better\n",
            "cats ---->  cat\n",
            "driving ---->  drive\n",
            "mice ---->  mice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(f\"{word}---------> {lemmatizer.lemmatize(word, wordnet.VERB)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mppo8d95cIq7",
        "outputId": "2c9889e6-0a55-4889-f72a-187c27a8f612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running---------> run\n",
            "flies---------> fly\n",
            "better---------> better\n",
            "cats---------> cat\n",
            "driving---------> drive\n",
            "mice---------> mice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#POS Tagging (Part-of-Speech Tagging)"
      ],
      "metadata": {
        "id": "HW-iCA_KhM8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS tagging is the process of assigning parts of speech (e.g, noun, verb, adjective) to each word in a sentence based on it's defination and context.\n",
        "\n",
        "it helps NLP modls understand grammatical structure and relationships between words."
      ],
      "metadata": {
        "id": "nn_FVuW4hPYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "iXgNl425ci8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfSACIiVh-3X",
        "outputId": "661b9897-7459-48cb-eb05-a44dd05c5b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the required NLTK resources\n",
        "nltk.download('punkt_tab')  # Tokenization\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # POS tagging\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72mDdK8MiGvl",
        "outputId": "60d6c6a2-7a84-440d-b9b2-4929b5d586cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "source": [
        "  # Tokenize the sentence\n",
        "sentence = 'The quick brown fox jumps over the lazy dog.'\n",
        "wordss = nltk.word_tokenize(sentence)\n",
        "print(wordss)\n",
        "\n",
        "    # Get part-of-speech tags\n",
        "pos_tag = nltk.pos_tag(wordss)\n",
        "print(pos_tag)\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiCz0dOnkikF",
        "outputId": "4a74bc5a-22b3-439e-e1ef-b157abc55f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tags Legend:\n",
        "\n",
        "PRP: Personal pronoun (e.g., I, he, she)\n",
        "\n",
        "VBZ: Verb, 3rd person singular present (e.g., sells, jumps)  \n",
        "\n",
        "NNS: Noun, plural (e.g., seashells)\n",
        "\n",
        "IN: Preposition or subordinating conjunction (e.g., by, on)\n",
        "\n",
        "DT: Determiner (e.g., the, a)\n",
        "\n",
        "NN: Noun, singular (e.g., cat, mat)\n",
        "\n",
        "VBP: Verb, non-3rd person singular present (e.g., love)\n",
        "\n",
        "VBG: Verb, gerund or present participle (e.g., sitting)\n",
        "\n",
        "NNP: Proper noun, singular (e.g., Python)"
      ],
      "metadata": {
        "id": "MtaV_zzVmEot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #tokenization\n",
        "nltk.download('average_percenpton_tagger_en') #part of speach\n",
        "nltk.download('maxent_ne_chunker_tab')  # Named Entity Chunker\n",
        "nltk.download('words')  # Words dataset for named entity recognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNbPYLMFiQUT",
        "outputId": "e2347178-7ee5-46e5-eee4-7f263cfedf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Error loading average_percenpton_tagger_en: Package\n",
            "[nltk_data]     'average_percenpton_tagger_en' not found in index\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize sentence\n",
        "sentence = 'Albert Einstein, the renowned physicist, developed the theory of relativity, which changed the understanding of space and time.'\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZIXOTTknXON",
        "outputId": "30e1aa29-2baa-4ec3-bca2-8bbfa0745c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Albert', 'Einstein', ',', 'the', 'renowned', 'physicist', ',', 'developed', 'the', 'theory', 'of', 'relativity', ',', 'which', 'changed', 'the', 'understanding', 'of', 'space', 'and', 'time', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get part of speech tags\n",
        "part_of_speech = nltk.pos_tag(words)\n",
        "print(part_of_speech)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABC7W2bvnmzN",
        "outputId": "1765e979-a98f-4470-d529-46affe65b5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Albert', 'NNP'), ('Einstein', 'NNP'), (',', ','), ('the', 'DT'), ('renowned', 'JJ'), ('physicist', 'NN'), (',', ','), ('developed', 'VBD'), ('the', 'DT'), ('theory', 'NN'), ('of', 'IN'), ('relativity', 'NN'), (',', ','), ('which', 'WDT'), ('changed', 'VBD'), ('the', 'DT'), ('understanding', 'NN'), ('of', 'IN'), ('space', 'NN'), ('and', 'CC'), ('time', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition (NER) - Identifying named entities like names, organizations, etc.\n",
        "named_entities = nltk.chunk.ne_chunk(part_of_speech)\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P71evbQkn2Yy",
        "outputId": "2fd4352b-8757-4b96-94b0-6289d1927bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Albert/NNP Einstein/NNP)\n",
            "  ,/,\n",
            "  the/DT\n",
            "  renowned/JJ\n",
            "  physicist/NN\n",
            "  ,/,\n",
            "  developed/VBD\n",
            "  the/DT\n",
            "  theory/NN\n",
            "  of/IN\n",
            "  relativity/NN\n",
            "  ,/,\n",
            "  which/WDT\n",
            "  changed/VBD\n",
            "  the/DT\n",
            "  understanding/NN\n",
            "  of/IN\n",
            "  space/NN\n",
            "  and/CC\n",
            "  time/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sentence segmentation\n",
        ". the input text split into one sentence.\n",
        "\"barack obama , the 44th president of the united states, met with elon musk in new city last week.\"\n",
        "\n",
        "# tokenization\n",
        ". the sentence is split into individual tokens (words and punctuation)\":\n",
        ".['Barack', 'Obama', ',', 'the', '44th', 'President', 'of', 'the', 'United', 'States', ',', 'met', 'with', 'Elon', 'Musk', 'in', 'New', 'York', 'City', 'last', 'week', '.']\n",
        "\n",
        "\n",
        "# part of speech tagging:\n",
        ". the tokens are tagged with parts of speech:\n",
        "Barack/NNP and Obama/NNP are proper nouns (named entities).\n",
        "\n",
        "44th/JJ is an adjective (ordinal number).\n",
        "\n",
        "President/NNP is a proper noun.\n",
        "met/VBD is a verb (past tense).\n",
        "\n",
        "New/NNP, York/NNP, City/NNP are proper nouns.\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        ". the named entities recognized are:\n",
        "\n",
        "Barack Obama as PERSON (a person).\n",
        "\n",
        "United States as GPE (Geopolitical Entity).\n",
        "\n",
        "Elon Musk as PERSON (a person).\n",
        "\n",
        "New York City as GPE (Geopolitical\n",
        " Entity)"
      ],
      "metadata": {
        "id": "nEAiOO6zo-Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "GKJlnrLSoFqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # For tokenization and sentence segmentation\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
        "nltk.download('maxent_ne_chunker')  # Named Entity Chunker\n",
        "nltk.download('words')  # Words dataset for named entity recognition\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfnf-jC-qKm1",
        "outputId": "f04d8f4b-8193-49e5-e92c-0177fa20c396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Barack Obama, the 44th President of the United States, met with Elon Musk in New York City last week.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(f'sentence: ' ,sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoAefqRCqjkx",
        "outputId": "13a84f1a-aff7-47a4-c682-e181b90e28c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence:  ['Barack Obama, the 44th President of the United States, met with Elon Musk in New York City last week.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Loop over each sentence and perform tokenization and NER\n",
        "for sentence in sentences:\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "\n",
        "    # Tokenization: Split sentence into words\n",
        "    word_tokens = nltk.word_tokenize(sentence)\n",
        "    print(\"Tokenized words:\", word_tokens)\n",
        "\n",
        "    # POS Tagging: Tag words with part of speech\n",
        "    pos_tags = nltk.pos_tag(word_tokens)\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "    # Named Entity Recognition (NER): Identify named entities\n",
        "    named_entities = nltk.chunk.ne_chunk(pos_tags)\n",
        "    print(\"Named Entities:\", named_entities)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYD-NvxQqv3j",
        "outputId": "889fd42c-f482-4826-90f6-594ec917704f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: Barack Obama, the 44th President of the United States, met with Elon Musk in New York City last week.\n",
            "Tokenized words: ['Barack', 'Obama', ',', 'the', '44th', 'President', 'of', 'the', 'United', 'States', ',', 'met', 'with', 'Elon', 'Musk', 'in', 'New', 'York', 'City', 'last', 'week', '.']\n",
            "POS Tags: [('Barack', 'NNP'), ('Obama', 'NNP'), (',', ','), ('the', 'DT'), ('44th', 'CD'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), (',', ','), ('met', 'VBD'), ('with', 'IN'), ('Elon', 'NNP'), ('Musk', 'NNP'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('City', 'NNP'), ('last', 'JJ'), ('week', 'NN'), ('.', '.')]\n",
            "Named Entities: (S\n",
            "  (PERSON Barack/NNP)\n",
            "  (GPE Obama/NNP)\n",
            "  ,/,\n",
            "  the/DT\n",
            "  44th/CD\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ,/,\n",
            "  met/VBD\n",
            "  with/IN\n",
            "  (PERSON Elon/NNP Musk/NNP)\n",
            "  in/IN\n",
            "  (GPE New/NNP York/NNP City/NNP)\n",
            "  last/JJ\n",
            "  week/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data cleaning conditions for Byte Pair Encoding\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T3GyGn_kyBhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Unicode Normalization\n",
        "2. Contraction Expansion\n",
        "3. Case Sensitivity Handling\n",
        "4. Tokenization Refinements (using spaCy)\n",
        "5. Removing Noise (special characters, URLs, emails)\n",
        "6. Filtering Low-Frequency Words\n",
        "7. Applying BPE"
      ],
      "metadata": {
        "id": "OtIscdpiyUGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoyeOlMhBSjx",
        "outputId": "a4e284aa-22fc-4a3e-a823-4864c26cc4e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import contractions\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import regex as re\n"
      ],
      "metadata": {
        "id": "L833OIHlyT37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import contractions\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import regex as re\n",
        "\n",
        "# Step 1: Text Normalization\n",
        "\n",
        "# Normalize unicode (removes accents and diacritics)\n",
        "def normalize_unicode(text):\n",
        "    normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    print(f\"Normalized Unicode: {normalized_text}\")\n",
        "    return normalized_text\n",
        "\n",
        "# Expand contractions (e.g., \"I'm\" → \"I am\")\n",
        "def expand_contractions(text):\n",
        "    expanded_text = contractions.fix(text)\n",
        "    print(f\"Expanded Contractions: {expanded_text}\")\n",
        "    return expanded_text\n",
        "\n",
        "# Handle case sensitivity (convert all text to lowercase)\n",
        "def normalize_case(text):\n",
        "    normalized_case_text = text.lower()\n",
        "    print(f\"Normalized Case: {normalized_case_text}\")\n",
        "    return normalized_case_text\n",
        "\n",
        "# Remove special characters, URLs, emails, and noise\n",
        "def remove_noise(text):\n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
        "    # Remove any characters that are not letters, numbers, or spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    print(f\"Text after Noise Removal: {text}\")\n",
        "    return text\n",
        "\n",
        "# Step 2: Tokenization with spaCy\n",
        "\n",
        "# Load spaCy model for tokenization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_with_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    print(f\"Tokens after Tokenization: {tokens}\")\n",
        "    return tokens\n",
        "\n",
        "# Step 3: Filtering Low-Frequency Words\n",
        "\n",
        "def filter_low_frequency_words(tokens, min_freq=2):\n",
        "    freq = Counter(tokens)\n",
        "    filtered_tokens = [word for word in tokens if freq[word] >= min_freq]\n",
        "    print(f\"Tokens after Filtering Low Frequency Words: {filtered_tokens}\")\n",
        "    return filtered_tokens\n",
        "\n",
        "# Step 4: Byte Pair Encoding (BPE)\n",
        "\n",
        "# Get frequency of pairs of adjacent words\n",
        "def get_stats(vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pair = (symbols[i], symbols[i + 1])\n",
        "            pairs[pair] += freq\n",
        "    print(f\"Pairs after get_stats: {pairs}\")\n",
        "    return pairs\n",
        "\n",
        "# Merge vocabulary using the best pair\n",
        "def merge_vocab(pair, vocab):\n",
        "    pattern = re.compile(r'\\b' + re.escape(' '.join(pair)) + r'\\b')\n",
        "    merged_vocab = {pattern.sub(''.join(pair), word): freq for word, freq in vocab.items()}\n",
        "    print(f\"Vocab after merge_vocab: {merged_vocab}\")\n",
        "    return merged_vocab\n",
        "\n",
        "# Apply Byte Pair Encoding (BPE)\n",
        "def byte_pair_encoding(vocab, num_merges=10):\n",
        "    for _ in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "    print(f\"Final Vocab after BPE: {vocab}\")\n",
        "    return vocab\n",
        "\n",
        "# Final processing function to apply all methods\n",
        "def process_string(text, min_freq=2, num_merges=10):\n",
        "    # Apply the advanced data cleaning methods\n",
        "    text = normalize_unicode(text)  # Normalize unicode\n",
        "    text = expand_contractions(text)  # Expand contractions\n",
        "    text = normalize_case(text)  # Normalize case\n",
        "    text = remove_noise(text)  # Remove noise (URLs, emails, special characters)\n",
        "\n",
        "    # Tokenize with spaCy\n",
        "    tokens = tokenize_with_spacy(text)\n",
        "\n",
        "    # Filter out low-frequency words\n",
        "    tokens = filter_low_frequency_words(tokens, min_freq)\n",
        "\n",
        "    # Compute word frequencies\n",
        "    word_freqs = Counter(tokens)\n",
        "    print(f\"Word Frequencies: {word_freqs}\")\n",
        "\n",
        "    # Apply Byte Pair Encoding (BPE)\n",
        "    vocab = byte_pair_encoding(word_freqs, num_merges)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Example text input\n",
        "text = \"\"\"This is 's example, 'll be there soon, 've been waiting, I love Python and the world, The cost is 500 dollars, and there are 3 apples. Hello, how are you? I am good! This is a sentence followed by trailing spaces. I love programming and coding. Visit my website at http://example.com or email me at test@example.com\"\"\"\n",
        "\n",
        "# Process the string and print the result\n",
        "result = process_string(text)\n",
        "print(f\"Final Result after all processing: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8yalkCHrSwP",
        "outputId": "361b6eab-d245-426f-8d10-97dea93205e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Unicode: This is 's example, 'll be there soon, 've been waiting, I love Python and the world, The cost is 500 dollars, and there are 3 apples. Hello, how are you? I am good! This is a sentence followed by trailing spaces. I love programming and coding. Visit my website at http://example.com or email me at test@example.com\n",
            "Expanded Contractions: This is 's example,  will be there soon, 've been waiting, I love Python and the world, The cost is 500 dollars, and there are 3 apples. Hello, how are you? I am good! This is a sentence followed by trailing spaces. I love programming and coding. Visit my website at http://example.com or email me at test@example.com\n",
            "Normalized Case: this is 's example,  will be there soon, 've been waiting, i love python and the world, the cost is 500 dollars, and there are 3 apples. hello, how are you? i am good! this is a sentence followed by trailing spaces. i love programming and coding. visit my website at http://example.com or email me at test@example.com\n",
            "Text after Noise Removal: this is s example  will be there soon ve been waiting i love python and the world the cost is 500 dollars and there are 3 apples hello how are you i am good this is a sentence followed by trailing spaces i love programming and coding visit my website at  or email me at \n",
            "Tokens after Tokenization: ['s', 'example', ' ', 'soon', 've', 'waiting', 'love', 'python', 'world', 'cost', '500', 'dollars', '3', 'apples', 'hello', 'good', 'sentence', 'followed', 'trailing', 'spaces', 'love', 'programming', 'coding', 'visit', 'website', ' ', 'email']\n",
            "Tokens after Filtering Low Frequency Words: [' ', 'love', 'love', ' ']\n",
            "Word Frequencies: Counter({' ': 2, 'love': 2})\n",
            "Pairs after get_stats: defaultdict(<class 'int'>, {})\n",
            "Final Vocab after BPE: Counter({' ': 2, 'love': 2})\n",
            "Final Result after all processing: Counter({' ': 2, 'love': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W4AbJOLLDgpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6WE9PGDmDggI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HPPD62WgDgWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HE8Kkg5vDgR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import contractions\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import regex as re\n",
        "\n",
        "# Step 1: Text Normalization\n",
        "\n",
        "# Normalize unicode (removes accents and diacritics)\n",
        "def normalize_unicode(text):\n",
        "    normalized_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "    print(f\"Normalized Unicode: {normalized_text}\")\n",
        "    return normalized_text\n",
        "\n",
        "# Expand contractions (e.g., \"I'm\" → \"I am\")\n",
        "def expand_contractions(text):\n",
        "    expanded_text = contractions.fix(text)\n",
        "    print(f\"Expanded Contractions: {expanded_text}\")\n",
        "    return expanded_text\n",
        "\n",
        "# Handle case sensitivity (convert all text to lowercase)\n",
        "def normalize_case(text):\n",
        "    normalized_case_text = text.lower()\n",
        "    print(f\"Normalized Case: {normalized_case_text}\")\n",
        "    return normalized_case_text\n",
        "\n",
        "# Remove special characters, URLs, emails, and noise\n",
        "def remove_noise(text):\n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)  # Remove emails\n",
        "    # Remove any characters that are not letters, numbers, or spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    print(f\"Text after Noise Removal: {text}\")\n",
        "    return text\n",
        "\n",
        "# Step 2: Tokenization with spaCy\n",
        "\n",
        "# Load spaCy model for tokenization\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_with_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token for token in doc if not token.is_stop and not token.is_punct]\n",
        "    token_texts = [token.text for token in tokens]\n",
        "    text_to_vector = {token.text: token.vector for token in tokens}\n",
        "    print(f\"Tokens after Tokenization: {token_texts}\")\n",
        "    return token_texts, text_to_vector\n",
        "\n",
        "# Step 3: Filtering Low-Frequency Words\n",
        "\n",
        "def filter_low_frequency_words(token_texts, min_freq=2):\n",
        "    freq = Counter(token_texts)\n",
        "    filtered_token_texts = [word for word in token_texts if freq[word] >= min_freq]\n",
        "    print(f\"Tokens after Filtering Low Frequency Words: {filtered_token_texts}\")\n",
        "    return filtered_token_texts\n",
        "\n",
        "# Step 4: Byte Pair Encoding (BPE)\n",
        "\n",
        "# Get frequency of pairs of adjacent words\n",
        "def get_stats(vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pair = (symbols[i], symbols[i + 1])\n",
        "            pairs[pair] += freq\n",
        "    print(f\"Pairs after get_stats: {pairs}\")\n",
        "    return pairs\n",
        "\n",
        "# Merge vocabulary using the best pair\n",
        "def merge_vocab(pair, vocab):\n",
        "    pattern = re.compile(r'\\b' + re.escape(' '.join(pair)) + r'\\b')\n",
        "    merged_vocab = {pattern.sub(''.join(pair), word): freq for word, freq in vocab.items()}\n",
        "    print(f\"Vocab after merge_vocab: {merged_vocab}\")\n",
        "    return merged_vocab\n",
        "\n",
        "# Apply Byte Pair Encoding (BPE)\n",
        "def byte_pair_encoding(vocab, num_merges=10):\n",
        "    for _ in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "    print(f\"Final Vocab after BPE: {vocab}\")\n",
        "    return vocab\n",
        "\n",
        "# Final processing function to apply all methods\n",
        "def process_string(text, min_freq=2, num_merges=10):\n",
        "    # Apply the advanced data cleaning methods\n",
        "    text = normalize_unicode(text)  # Normalize unicode\n",
        "    text = expand_contractions(text)  # Expand contractions\n",
        "    text = normalize_case(text)  # Normalize case\n",
        "    text = remove_noise(text)  # Remove noise (URLs, emails, special characters)\n",
        "\n",
        "    # Tokenize with spaCy\n",
        "    token_texts, text_to_vector = tokenize_with_spacy(text)\n",
        "\n",
        "    # Filter out low-frequency words\n",
        "    filtered_token_texts = filter_low_frequency_words(token_texts, min_freq)\n",
        "\n",
        "    # Compute word frequencies\n",
        "    word_freqs = Counter(filtered_token_texts)\n",
        "    print(f\"Word Frequencies: {word_freqs}\")\n",
        "\n",
        "    # Apply Byte Pair Encoding (BPE)\n",
        "    vocab = byte_pair_encoding(word_freqs, num_merges)\n",
        "\n",
        "    # Generate and print embeddings for the final vocabulary\n",
        "    print(\"\\nEmbeddings:\")\n",
        "    for word in vocab.keys():\n",
        "        if word in text_to_vector:\n",
        "            embedding = text_to_vector[word]\n",
        "            print(f\"{word}: {embedding[:5]}... (total {len(embedding)} dimensions)\")\n",
        "        else:\n",
        "            print(f\"{word}: No embedding found\")\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Example text input\n",
        "text = \"\"\"This is 's example, 'll be there soon, 've been waiting, I love Python and the world, The cost is 500 dollars, and there are 3 apples. Hello, how are you? I am good! This is a sentence followed by trailing spaces. I love programming and coding. Visit my website at http://example.com or email me at test@example.com\"\"\"\n",
        "\n",
        "# Process the string and print the result\n",
        "result = process_string(text, min_freq=1)  # Set min_freq=1 to include all tokens\n",
        "print(f\"Final Result after all processing: {result}\")"
      ],
      "metadata": {
        "id": "nYXvZ_FtUVDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd0b3a9-4e6b-4504-aaf0-5ca813c2e978"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Unicode: This is 's example, 'll be there soon, 've been waiting, I love Python and the world, The cost is 500 dollars, and there are 3 apples. Hello, how are you? I am good! This is a sentence followed by trailing spaces. I love programming and coding. Visit my website at http://example.com or email me at test@example.com\n",
            "Expanded Contractions: This is 's example,  will be there soon, 've been waiting, I love Python and the world, The cost is 500 dollars, and there are 3 apples. Hello, how are you? I am good! This is a sentence followed by trailing spaces. I love programming and coding. Visit my website at http://example.com or email me at test@example.com\n",
            "Normalized Case: this is 's example,  will be there soon, 've been waiting, i love python and the world, the cost is 500 dollars, and there are 3 apples. hello, how are you? i am good! this is a sentence followed by trailing spaces. i love programming and coding. visit my website at http://example.com or email me at test@example.com\n",
            "Text after Noise Removal: this is s example  will be there soon ve been waiting i love python and the world the cost is 500 dollars and there are 3 apples hello how are you i am good this is a sentence followed by trailing spaces i love programming and coding visit my website at  or email me at \n",
            "Tokens after Tokenization: ['s', 'example', ' ', 'soon', 've', 'waiting', 'love', 'python', 'world', 'cost', '500', 'dollars', '3', 'apples', 'hello', 'good', 'sentence', 'followed', 'trailing', 'spaces', 'love', 'programming', 'coding', 'visit', 'website', ' ', 'email']\n",
            "Tokens after Filtering Low Frequency Words: ['s', 'example', ' ', 'soon', 've', 'waiting', 'love', 'python', 'world', 'cost', '500', 'dollars', '3', 'apples', 'hello', 'good', 'sentence', 'followed', 'trailing', 'spaces', 'love', 'programming', 'coding', 'visit', 'website', ' ', 'email']\n",
            "Word Frequencies: Counter({' ': 2, 'love': 2, 's': 1, 'example': 1, 'soon': 1, 've': 1, 'waiting': 1, 'python': 1, 'world': 1, 'cost': 1, '500': 1, 'dollars': 1, '3': 1, 'apples': 1, 'hello': 1, 'good': 1, 'sentence': 1, 'followed': 1, 'trailing': 1, 'spaces': 1, 'programming': 1, 'coding': 1, 'visit': 1, 'website': 1, 'email': 1})\n",
            "Pairs after get_stats: defaultdict(<class 'int'>, {})\n",
            "Final Vocab after BPE: Counter({' ': 2, 'love': 2, 's': 1, 'example': 1, 'soon': 1, 've': 1, 'waiting': 1, 'python': 1, 'world': 1, 'cost': 1, '500': 1, 'dollars': 1, '3': 1, 'apples': 1, 'hello': 1, 'good': 1, 'sentence': 1, 'followed': 1, 'trailing': 1, 'spaces': 1, 'programming': 1, 'coding': 1, 'visit': 1, 'website': 1, 'email': 1})\n",
            "\n",
            "Embeddings:\n",
            "s: [ 0.1694575   0.34592855  0.09320211  0.5896082  -0.4289767 ]... (total 96 dimensions)\n",
            "example: [ 0.1062118  -1.1063172   0.28210837 -0.775419    0.31804365]... (total 96 dimensions)\n",
            " : [-0.79926014 -0.5034056   1.709035   -1.4242048   0.23782174]... (total 96 dimensions)\n",
            "soon: [ 0.13716458 -0.52091324 -1.0028589  -1.1714407  -0.64733577]... (total 96 dimensions)\n",
            "ve: [-1.2350115  -1.4512284   1.393496   -1.4079564   0.09087359]... (total 96 dimensions)\n",
            "waiting: [ 0.04498197  1.2553824   0.3294173   1.6807542  -0.5898819 ]... (total 96 dimensions)\n",
            "love: [-0.25814885 -0.27393407 -0.19703883 -0.53565454 -0.48813182]... (total 96 dimensions)\n",
            "python: [-0.09240355  0.45349994  0.07686354 -0.28242406 -0.26613885]... (total 96 dimensions)\n",
            "world: [-0.601226   -0.23292175 -0.78542274  0.42063725 -0.21406533]... (total 96 dimensions)\n",
            "cost: [ 0.31976116 -0.27636927 -0.8178972   0.04894321 -0.42406768]... (total 96 dimensions)\n",
            "500: [-0.2595137   0.19787732  1.06756     0.39333898  1.4496926 ]... (total 96 dimensions)\n",
            "dollars: [ 0.3647497   0.8415576   0.06637755 -0.7152372   0.3879503 ]... (total 96 dimensions)\n",
            "3: [0.13549119 0.315872   1.9072536  0.17166324 1.7030892 ]... (total 96 dimensions)\n",
            "apples: [-0.0914915   0.80754244 -0.02928564  0.71748304  1.6233968 ]... (total 96 dimensions)\n",
            "hello: [ 0.19013898 -0.36941212  0.4410131   0.49020925 -1.5072579 ]... (total 96 dimensions)\n",
            "good: [ 0.53611064 -0.33842814  0.40256628 -0.32785624 -0.28772914]... (total 96 dimensions)\n",
            "sentence: [ 0.10643949 -0.41062737 -0.10339411 -0.43623197  0.7481145 ]... (total 96 dimensions)\n",
            "followed: [ 1.4486905   0.15200934 -0.40029174 -0.24025665 -1.2012026 ]... (total 96 dimensions)\n",
            "trailing: [0.6957092  0.8974338  0.2912299  1.0712256  0.43951464]... (total 96 dimensions)\n",
            "spaces: [ 0.50812304  0.88754094 -0.3895345  -0.15167008  1.014308  ]... (total 96 dimensions)\n",
            "programming: [-0.6495561   1.1791197  -0.08406401  0.66175646 -0.50072616]... (total 96 dimensions)\n",
            "coding: [0.3331108  0.10657761 0.82394814 0.93655956 0.19335662]... (total 96 dimensions)\n",
            "visit: [-0.69935024 -1.0216099  -0.24970219 -0.0456473  -0.7630866 ]... (total 96 dimensions)\n",
            "website: [-1.0651482  -0.09061518  0.47460508 -0.2702465  -0.07767589]... (total 96 dimensions)\n",
            "email: [-0.73101294 -0.48246592 -0.54739296  0.74197036  0.13019298]... (total 96 dimensions)\n",
            "Final Result after all processing: Counter({' ': 2, 'love': 2, 's': 1, 'example': 1, 'soon': 1, 've': 1, 'waiting': 1, 'python': 1, 'world': 1, 'cost': 1, '500': 1, 'dollars': 1, '3': 1, 'apples': 1, 'hello': 1, 'good': 1, 'sentence': 1, 'followed': 1, 'trailing': 1, 'spaces': 1, 'programming': 1, 'coding': 1, 'visit': 1, 'website': 1, 'email': 1})\n"
          ]
        }
      ]
    }
  ]
}